# LinearRegression1D_Bias

> **A self-evolving 1D linear regression model where bias learns context instead of remaining constant.**

---

## ğŸš€ Motivation

Classical linear regression assumes:

```
y = wx + b
```

where the **bias (b)** is a constant scalar.

This assumption breaks down in real-world systems where:

* regime shifts exist
* offsets change with input context
* asymmetric behavior appears across domains

This project introduces the **context-aware bias**:

```
y = wx + b(x)
```

where bias itself as  **learned as a smooth function of input**.

---

## ğŸ§  Core Idea

* Keep the **linear slope (w)** global and totally interpretable
* Let the **bias evolve with input context**
* Preserve simplicity while capturing non-linear offsets

No neural networks.
No feature engineering.
No piecewise heuristics.

Just a better modeling assumption.

---

## ğŸ“ Mathematical Formulation

The model is defined as:

```
yÌ‚ = wÂ·x + Î±Â·tanh(Î²x + Î³)
```

Where:

* `w` â†’ global linear slope
* `Î±` â†’ bias amplitude
* `Î²` â†’ bias sharpness
* `Î³` â†’ bias shift

The **tanh-based bias** allows smooth, differentiable transitions across regimes.

---

## ğŸ“Š What the Visualization Shows

The generated plot decomposes the model into:

* ğŸ”µ **Linear Component** â†’ `wÂ·x`
* ğŸŸ  **Bias Function** â†’ `Î±Â·tanh(Î²x + Î³)`
* ğŸŸ¢ **Final Prediction** â†’ sum of both

This explicit decomposition is critical for:

* interpretability
* debugging
* research analysis

---

## ğŸ—‚ Project Structure

```
Self-Evolving-1D-Linear-Regression/
â”‚
â”œâ”€â”€ main.py                  # Training + visualization entry point
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ model.py             # LinearRegression1D_Bias definition
â”‚   â”œâ”€â”€ optimizer.py         # Gradient descent updates
â”‚
â”œâ”€â”€ visuals/
â”‚   â””â”€â”€ plot_components.py   # Component-wise visualization
â”‚
â””â”€â”€ README.md
```

---

## â–¶ï¸ How to Run

### 1. Install dependencies

```
pip install numpy matplotlib
```

### 2. Run the experiment

```
python main.py
```

This will:

* generate synthetic data with regime shifts
* train the model via gradient descent
* plot model components

---

## ğŸ“ˆ Example Output

You will see a plot with:

* a straight line (global trend)
* a smooth bias curve
* a final prediction adapting across input space

This behavior **cannot be achieved with standard linear regression**.

---

## ğŸ”¬ Research Direction

This project serves as a foundation for an upcoming **research paper** exploring:

* context-aware bias in linear models
* interpretability-preserving alternatives to neural networks
* regime-aware regression for real-world data

---

## ğŸ§ª Use Cases

* Manufacturing & sensor calibration
* Economics & policy modeling
* System drift correction
* Explainable AI pipelines

---

## ğŸ¤ Contributions

Ideas, critiques, and extensions are welcome.

If you build on this concept, please cite or reference the project.

---

## ğŸ“œ License

MIT License â€” free to use, modify, and build upon.

---

â­ If this idea challenges how you think about linear models, consider starring the repo.
